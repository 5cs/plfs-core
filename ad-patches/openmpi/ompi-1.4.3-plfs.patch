--- openmpi-1.4.3/ompi/mca/io/romio/romio/adio/ad_plfs/ad_plfs.c	1969-12-31 16:00:00.000000000 -0800
+++ manio-ompi-1.4.3/ompi/mca/io/romio/romio/adio/ad_plfs/ad_plfs.c	2012-11-29 14:17:04.225331098 -0800
@@ -0,0 +1,343 @@
+/* -*- Mode: C; c-basic-offset:4 ; -*- */
+/*
+ *   $Id: ad_plfs.c,v 1.1 2010/11/29 19:59:01 adamm Exp $
+ *
+ *   Copyright (C) 2001 University of Chicago.
+ *   See COPYRIGHT notice in top-level directory.
+ */
+
+#include "ad_plfs.h"
+
+#ifdef ROMIO_CRAY
+#include "../ad_cray/ad_cray.h"
+#endif /* ROMIO_CRAY */
+
+/* adioi.h has the ADIOI_Fns_struct define */
+#include "adioi.h"
+
+struct ADIOI_Fns_struct ADIO_PLFS_operations = {
+    ADIOI_PLFS_Open, /* Open */
+#ifdef ROMIO_CRAY
+    /* BEGIN CRAY ADDITION */
+    /*
+     * The ROMIO that PLFS was developed against is old and doesn't have
+     * the following entry or Feature
+     */
+    ADIOI_GEN_OpenColl,  /* OpenColl */
+    /* END CRAY ADDITION */
+#endif
+    ADIOI_PLFS_ReadContig, /* ReadContig */
+    ADIOI_PLFS_WriteContig, /* WriteContig */
+#ifdef ROMIO_CRAY
+    ADIOI_CRAY_ReadStridedColl, /* ReadStridedColl */
+    ADIOI_CRAY_WriteStridedColl, /* WriteStridedColl */
+#else
+    ADIOI_GEN_ReadStridedColl, /* ReadStridedColl */
+    ADIOI_GEN_WriteStridedColl, /* WriteStridedColl */
+#endif /* ROMIO_CRAY */
+    ADIOI_GEN_SeekIndividual, /* SeekIndividual */
+    ADIOI_PLFS_Fcntl, /* Fcntl */
+    ADIOI_PLFS_SetInfo, /* SetInfo */
+    ADIOI_GEN_ReadStrided, /* ReadStrided */
+    ADIOI_GEN_WriteStrided, /* WriteStrided */
+    ADIOI_PLFS_Close, /* Close */
+    ADIOI_FAKE_IreadContig, /* IreadContig */
+    ADIOI_FAKE_IwriteContig, /* IwriteContig */
+    ADIOI_FAKE_IODone, /* ReadDone */
+    ADIOI_FAKE_IODone, /* WriteDone */
+    ADIOI_FAKE_IOComplete, /* ReadComplete */
+    ADIOI_FAKE_IOComplete, /* WriteComplete */
+    ADIOI_FAKE_IreadStrided, /* IreadStrided */
+    ADIOI_FAKE_IwriteStrided, /* IwriteStrided */
+    ADIOI_PLFS_Flush, /* Flush */
+    ADIOI_PLFS_Resize, /* Resize */
+    ADIOI_PLFS_Delete, /* Delete */
+#ifdef ROMIO_CRAY
+    /* BEGIN CRAY ADDITION */
+    /*
+     * The ROMIO that PLFS was developed against is old and doesn't have
+     * the following entry or OpenColl
+     */
+     ADIOI_GEN_Feature, /* Features */
+    /* END CRAY ADDITION */
+#endif
+};
+
+int plfs_protect_all(const char *file, MPI_Comm comm) {
+    int rank;
+    MPI_Comm_rank(comm,&rank);
+    return plfs_protect(file,rank);
+}
+
+int ad_plfs_amode( int access_mode )
+{
+    int amode = 0; // O_META;
+    if (access_mode & ADIO_RDONLY) {
+        amode = amode | O_RDONLY;
+    }
+    if (access_mode & ADIO_WRONLY) {
+        amode = amode | O_WRONLY;
+    }
+    if (access_mode & ADIO_RDWR) {
+        amode = amode | O_RDWR;
+    }
+    if (access_mode & ADIO_EXCL) {
+        amode = amode | O_EXCL;
+    }
+    return amode;
+}
+
+
+
+int ad_plfs_hints(ADIO_File fd, int rank, char *hint)
+{
+    int hint_value=0,flag,resultlen,mpi_ret;
+    char *value;
+    char err_buffer[MPI_MAX_ERROR_STRING];
+    // get the value of broadcast
+    value = (char *) ADIOI_Malloc((MPI_MAX_INFO_VAL+1));
+    mpi_ret=MPI_Info_get(fd->info,hint,MPI_MAX_INFO_VAL,value,&flag);
+    // If there is an error on the info get the rank and the error message
+    if(mpi_ret!=MPI_SUCCESS) {
+        MPI_Error_string(mpi_ret,err_buffer,&resultlen);
+        MPI_Abort(MPI_COMM_WORLD,MPI_ERR_IO);
+        return -1;
+    } else {
+        if(flag) {
+            hint_value = atoi(value);
+        }
+    }
+    ADIOI_Free(value);
+    return hint_value;
+}
+
+void malloc_check(void *test_me,int rank)
+{
+    if(!test_me) {
+        plfs_debug("Rank %d failed a malloc check\n");
+        MPI_Abort(MPI_COMM_WORLD,MPI_ERR_IO);
+    }
+}
+
+void check_stream(int size,int rank)
+{
+    if(size<0) {
+        plfs_debug("Rank %d had a stream with a negative return size\n");
+        MPI_Abort(MPI_COMM_WORLD,MPI_ERR_IO);
+    }
+}
+
+/* --BEGIN CRAY ADDITION-- */
+
+/*
+ * If the PLFS library is not linked into the executable, these weak
+ * stubs will be called to issue an informative message and then abort.
+ */
+
+static void no_link_abort(void)
+{
+    FPRINTF(stderr,"A PLFS routine was called but the PLFS library "
+                   "is not linked into the program\n");
+    MPI_Abort(MPI_COMM_WORLD, __LINE__);
+}
+
+int plfs_close(Plfs_fd *,pid_t,uid_t,int open_flags,Plfs_close_opt *close_opt)  __attribute__ ((weak));
+int plfs_close(Plfs_fd *fd,pid_t pid,uid_t uid,int open_flags,Plfs_close_opt *close_opt)
+{
+    no_link_abort();
+    return -1; /* never gets here */
+}
+
+int plfs_create( const char *path, mode_t mode, int flags, pid_t pid )  __attribute__ ((weak));
+int plfs_create( const char *path, mode_t mode, int flags, pid_t pid )
+{
+    no_link_abort();
+    return -1; /* never gets here */
+}
+
+void plfs_debug( const char *format, ... )  __attribute__ ((weak));
+void plfs_debug( const char *format, ... )
+{
+    no_link_abort();
+}
+
+int plfs_expand_path(const char *logical,char **physical)  __attribute__ ((weak));
+int plfs_expand_path(const char *logical,char **physical)
+{
+    no_link_abort();
+    return -1; /* never gets here */
+}
+
+int plfs_flatten_index( Plfs_fd *, const char *path )  __attribute__ ((weak));
+int plfs_flatten_index( Plfs_fd *fd, const char *path )
+{
+    no_link_abort();
+    return -1; /* never gets here */
+}
+
+int plfs_getattr(Plfs_fd *, const char *path, struct stat *st, int size_only)  __attribute__ ((weak));
+int plfs_getattr(Plfs_fd *fd, const char *path, struct stat *st, int size_only)
+{
+    no_link_abort();
+    return -1; /* never gets here */
+}
+
+size_t plfs_gethostdir_id(char *)  __attribute__ ((weak));
+size_t plfs_gethostdir_id(char *id)
+{
+    no_link_abort();
+    return 1; /* never gets here */
+}
+
+char *plfs_gethostname()  __attribute__ ((weak));
+char *plfs_gethostname()
+{
+    no_link_abort();
+    return NULL; /* never gets here */
+}
+
+int plfs_hostdir_rddir(void **index_stream,char *targets,
+        int rank,char * top_level)  __attribute__ ((weak));
+int plfs_hostdir_rddir(void **index_stream,char *targets,
+        int rank,char * top_level)
+{
+    no_link_abort();
+    return -1; /* never gets here */
+}
+
+int plfs_hostdir_zero_rddir(void **entries,const char* path,int rank)  __attribute__ ((weak));
+int plfs_hostdir_zero_rddir(void **entries,const char* path,int rank)
+{
+    no_link_abort();
+    return -1; /* never gets here */
+}
+
+int plfs_index_stream(Plfs_fd **pfd, char ** buffer)  __attribute__ ((weak));
+int plfs_index_stream(Plfs_fd **pfd, char ** buffer)
+{
+    no_link_abort();
+    return -1; /* never gets here */
+}
+
+int plfs_merge_indexes(Plfs_fd **pfd, char *index_streams,
+                        int *index_sizes, int procs)  __attribute__ ((weak));
+int plfs_merge_indexes(Plfs_fd **pfd, char *index_streams,
+                        int *index_sizes, int procs)
+{
+    no_link_abort();
+    return -1; /* never gets here */
+}
+
+int plfs_open( Plfs_fd **, const char *path,
+        int flags, pid_t pid, mode_t , Plfs_open_opt *open_opt)  __attribute__ ((weak));
+int plfs_open( Plfs_fd **fd, const char *path,
+        int flags, pid_t pid, mode_t mode, Plfs_open_opt *open_opt)
+{
+    no_link_abort();
+    return -1; /* never gets here */
+}
+
+int plfs_parindex_read(int rank, int ranks_per_comm,void *index_files,
+        void **index_stream,char *top_level)  __attribute__ ((weak));
+int plfs_parindex_read(int rank, int ranks_per_comm,void *index_files,
+        void **index_stream,char *top_level)
+{
+    no_link_abort();
+    return -1; /* never gets here */
+}
+
+int plfs_parindexread_merge(const char *path,char *index_streams,
+    int *index_sizes, int procs, void **index_stream)  __attribute__ ((weak));
+int plfs_parindexread_merge(const char *path,char *index_streams,
+    int *index_sizes, int procs, void **index_stream)
+{
+    no_link_abort();
+    return -1; /* never gets here */
+}
+
+ssize_t plfs_read( Plfs_fd *, char *buf, size_t size, off_t offset ) 
+    __attribute__ ((weak));
+ssize_t plfs_read( Plfs_fd *fd, char *buf, size_t size, off_t offset )
+{
+    no_link_abort();
+    return -1; /* never gets here */
+}
+
+int plfs_sync( Plfs_fd *)  __attribute__ ((weak));
+int plfs_sync( Plfs_fd *fd)
+{
+    no_link_abort();
+    return -1; /* never gets here */
+}
+
+int plfs_trunc( Plfs_fd *, const char *path, off_t, int open_file )  
+    __attribute__ ((weak));
+int plfs_trunc( Plfs_fd *fd, const char *path, off_t off, int open_file )
+{
+    no_link_abort();
+    return -1; /* never gets here */
+}
+
+int plfs_unlink( const char *path )  __attribute__ ((weak));
+int plfs_unlink( const char *path )
+{
+    no_link_abort();
+    return -1; /* never gets here */
+}
+
+ssize_t plfs_write( Plfs_fd *, const char *, size_t, off_t, pid_t )  
+    __attribute__ ((weak));
+ssize_t plfs_write( Plfs_fd *fd,
+    const char *buf, size_t size, off_t off, pid_t pid)
+{
+    no_link_abort();
+    return -1; /* never gets here */
+}
+
+int plfs_protect(const char *, pid_t)
+     __attribute__ ((weak));
+int plfs_protect(const char *path, pid_t pid)
+{
+    no_link_abort();
+    return -1; /* never gets here */
+}
+
+int plfs_query( Plfs_fd *, size_t *, size_t *, size_t *, int *)
+     __attribute__ ((weak));
+int plfs_query( Plfs_fd *fd, size_t *writers, size_t *readers,
+                    size_t *bytes_written, int *lazy_stat)
+{
+    no_link_abort();
+    return -1; /* never gets here */
+}
+
+plfs_filetype plfs_get_filetype(const char *path)
+     __attribute__ ((weak));
+plfs_filetype plfs_get_filetype(const char *path)
+{
+    no_link_abort();
+    return -1; /* never gets here */
+}
+
+
+int plfs_compress( char *compr_index, unsigned long *index_size_1,
+        char *index_stream, unsigned long index_size_0)
+    __attribute__ ((weak));
+int plfs_compress( char *compr_index, unsigned long *index_size_1,
+        char *index_stream, unsigned long index_size_0)
+{
+    no_link_abort();
+    return -1; /* never gets here */
+}
+
+int plfs_uncompress( char *index_stream, unsigned long *uncompr_len,
+      char *compr_index, unsigned long index_size_1)
+    __attribute__ ((weak));
+int plfs_uncompress( char *index_stream, unsigned long *uncompr_len,
+      char *compr_index, unsigned long index_size_1)
+{
+    no_link_abort();
+    return -1; /* never gets here */
+}
+
+/* --END CRAY ADDITION-- */
--- openmpi-1.4.3/ompi/mca/io/romio/romio/adio/ad_plfs/ad_plfs_close.c	1969-12-31 16:00:00.000000000 -0800
+++ manio-ompi-1.4.3/ompi/mca/io/romio/romio/adio/ad_plfs/ad_plfs_close.c	2012-11-29 14:17:04.225331098 -0800
@@ -0,0 +1,236 @@
+/* -*- Mode: C; c-basic-offset:4 ; -*- */
+/*
+ *   $Id: ad_plfs_close.c,v 1.9 2004/10/04 15:51:01 robl Exp $
+ *
+ *   Copyright (C) 1997 University of Chicago.
+ *   See COPYRIGHT notice in top-level directory.
+ */
+
+#include "ad_plfs.h"
+
+#include <sys/types.h>
+#include <sys/stat.h>
+#include <unistd.h>
+
+int flatten_then_close(ADIO_File, Plfs_fd *fd,int rank,int amode,int procs,
+                       Plfs_close_opt *close_opt,const char *filename,uid_t);
+void check_error(int err,int rank);
+void reduce_meta(ADIO_File, Plfs_fd *fd,const char *filename,
+                 Plfs_close_opt *close_opt, int rank);
+
+
+void ADIOI_PLFS_Close(ADIO_File fd, int *error_code)
+{
+    int err, rank, amode,procs;
+    static char myname[] = "ADIOI_PLFS_CLOSE";
+    uid_t uid = geteuid();
+    Plfs_close_opt close_opt;
+    close_opt.pinter=PLFS_MPIIO;
+    int flatten=0;
+    plfs_debug("%s: begin\n", myname );
+    MPI_Comm_rank( fd->comm, &rank );
+    MPI_Comm_size( fd->comm, &procs);
+    #if 0 /* TODO: original code - probably an error, but it is never used. */
+      close_opt.num_procs = &procs;
+    #else
+      close_opt.num_procs = procs;
+    #endif
+    close_opt.num_procs = procs;
+    amode = ad_plfs_amode( fd->access_mode );
+    if(fd->fs_ptr==NULL) {
+        // ADIO does a weird thing where it
+        // passes CREAT to just 0 and then
+        // immediately closes.  When we handle
+        // the CREAT, we put a NULL in
+        *error_code = MPI_SUCCESS;
+        return;
+    }
+    double start_time,end_time;
+    start_time=MPI_Wtime();
+    if (plfs_get_filetype(fd->filename) != CONTAINER) {
+        err = plfs_close(fd->fs_ptr, rank, uid,amode,NULL);
+    }else{
+        flatten = ad_plfs_hints (fd , rank, "plfs_flatten_close");
+        if(flatten && fd->access_mode!=ADIO_RDONLY) {
+            // flatten on close takes care of calling plfs_close and setting
+            // up the close_opt
+            close_opt.valid_meta=0;
+            plfs_debug("Rank: %d in flatten then close\n",rank);
+            err = flatten_then_close(fd, fd->fs_ptr, rank, amode, procs, &close_opt,
+                                     fd->filename,uid);
+        } else {
+            // for ADIO, just 0 creates the openhosts and the meta dropping
+            // Grab the last offset and total bytes from all ranks and reduce to max
+            plfs_debug("Rank: %d in regular close\n",rank);
+            if(fd->access_mode!=ADIO_RDONLY) {
+                reduce_meta(fd, fd->fs_ptr, fd->filename, &close_opt, rank);
+            }
+            err = plfs_close(fd->fs_ptr, rank, uid,amode,&close_opt);
+        }
+    } 
+    end_time=MPI_Wtime();
+    plfs_debug("%d: close time: %.2f\n", rank,end_time-start_time);
+    fd->fs_ptr = NULL;
+    if (err < 0 ) {
+        *error_code = MPIO_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE,
+                                           myname, __LINE__, MPI_ERR_IO,
+                                           "**io",
+                                           "**io %s", strerror(-err));
+    } else {
+        *error_code = MPI_SUCCESS;
+    }
+}
+
+
+int flatten_then_close(ADIO_File afd, Plfs_fd *fd,int rank,int amode,int procs,
+                       Plfs_close_opt *close_opt, const char *filename,
+                       uid_t uid)
+{
+    int index_size,err,index_total_size=0,streams_malloc=1,stop_buffer=0;
+    int *index_sizes,*index_disp;
+    char *index_stream,*index_streams;
+    double start_time,end_time;
+    // Get the index stream from the local index
+    index_size=plfs_index_stream(&(fd),&index_stream);
+    // Malloc space to receive all of the index sizes
+    // Do all procs need to do this? I think not
+    if(!rank) {
+        index_sizes=(int *)malloc(procs*sizeof(int));
+        if(!index_sizes) {
+            plfs_debug("Malloc failed:index size gather\n");
+            MPI_Abort(MPI_COMM_WORLD,MPI_ERR_IO);
+        }
+    }
+    if(!rank) {
+        start_time=MPI_Wtime();
+    }
+    // Perform the gather of all index sizes to set up our vector call
+    MPI_Gather(&index_size,1,MPI_INT,index_sizes,1,MPI_INT,0,afd->comm);
+    // Figure out how much space we need and then malloc if we are root
+    if(!rank) {
+        end_time=MPI_Wtime();
+        plfs_debug("Gather of index sizes time:%.12f\n"
+                   ,end_time-start_time);
+        int count;
+        // Malloc space for out displacements
+        index_disp=malloc(procs*sizeof(int));
+        if(!index_disp) {
+            plfs_debug("Displacements malloc has failed\n");
+            MPI_Abort(MPI_COMM_WORLD,MPI_ERR_IO);
+        }
+        for(count=0; count<procs; count++) {
+            index_disp[count]=index_total_size;
+            // Calculate the size of the index
+            index_total_size+=index_sizes[count];
+            if(index_sizes[count]==-1) {
+                plfs_debug("Rank %d had an index that wasn't buffered\n",count);
+                stop_buffer=1;
+            }
+        }
+        plfs_debug("Total size of indexes %d\n",index_total_size);
+        if(!stop_buffer) {
+            index_streams=(char *)malloc((index_total_size*sizeof(char)));
+            if(!index_streams) {
+                plfs_debug("Malloc failed:index streams\n");
+                streams_malloc=0;
+            }
+        }
+    }
+    err=MPI_Bcast(&streams_malloc,1,MPI_INT,0,afd->comm);
+    check_error(err,rank);
+    err=MPI_Bcast(&stop_buffer,1,MPI_INT,0,afd->comm);
+    check_error(err,rank);
+    if(!rank) {
+        start_time=MPI_Wtime();
+    }
+    // Gather all of the subindexes only if malloc succeeded
+    // and no one stopped buffering
+    if( streams_malloc && !stop_buffer) {
+        MPI_Gatherv(index_stream,index_size,MPI_CHAR,index_streams,
+                    index_sizes,index_disp,MPI_CHAR,0,afd->comm);
+    }
+    if(!rank) {
+        end_time=MPI_Wtime();
+        plfs_debug("Gatherv of indexes:%.12f\n"
+                   ,end_time-start_time);
+    }
+    // We are root lets combine all of our subindexes
+    if(!rank && streams_malloc && !stop_buffer) {
+        plfs_debug("About to merge indexes for %s\n",filename);
+        start_time=MPI_Wtime();
+        plfs_merge_indexes(&(fd),index_streams,index_sizes,procs);
+        end_time=MPI_Wtime();
+        plfs_debug("Finished merging indexes time:%.12f\n"
+                   ,end_time-start_time);
+        start_time=MPI_Wtime();
+        plfs_flatten_index(fd,filename);
+        end_time=MPI_Wtime();
+        plfs_debug("Finished flattening time:%.12f\n"
+                   ,end_time-start_time);
+    }
+    if(stop_buffer) {
+        reduce_meta(afd, fd, filename, close_opt, rank);
+    }
+    // Close normally
+    // This should be fine before the previous if statement
+    err = plfs_close(fd, rank, uid, amode,close_opt);
+    if(index_size>0) {
+        free(index_stream);
+    }
+    if(!rank) {
+        // Only root needs to complete these frees
+        free(index_sizes);
+        free(index_disp);
+        if(streams_malloc && !stop_buffer) {
+            free(index_streams);
+        }
+    }
+    // Everyone needs to free their index stream
+    // Root doesn't really need to make this call
+    // Could take out the plfs_index_stream call for root
+    // This is causing errors does the free to index streams clean this up?
+    return err;
+}
+
+void reduce_meta(ADIO_File afd, Plfs_fd *fd,const char *filename,
+                 Plfs_close_opt *close_opt, int rank)
+{
+    int BLKSIZE=512;
+    struct stat buf;
+    size_t glbl_tot_byt=0;
+    int size_only=1, lazy_stat=1;
+    long long tmp_buf;
+    plfs_query(fd, NULL, NULL, NULL, &lazy_stat);
+    if (lazy_stat == 0) {
+        // every rank calls plfs_sync to flush in-memory index.
+        plfs_sync(fd);
+        plfs_barrier(afd->comm,rank);
+        // rank 0 does slow stat, need not BCAST here
+        if (rank == 0) {
+            size_only = 0;
+            plfs_getattr(fd, filename, &buf, size_only);
+            close_opt->last_offset = (size_t)buf.st_size;
+            glbl_tot_byt = (size_t)buf.st_blocks;
+        }
+    } else {
+        plfs_getattr(fd, filename, &buf, size_only);
+        MPI_Reduce(&(buf.st_size),&tmp_buf,1,MPI_LONG_LONG,MPI_MAX,0,afd->comm);
+        close_opt->last_offset = (off_t)tmp_buf;
+        MPI_Reduce(&(buf.st_blocks),&tmp_buf,1,MPI_LONG_LONG,MPI_SUM,
+                   0,afd->comm);
+        glbl_tot_byt = (size_t)tmp_buf;
+    }
+    close_opt->total_bytes=glbl_tot_byt*BLKSIZE;
+    close_opt->valid_meta=1;
+}
+
+void check_error(int err,int rank)
+{
+    if(err != MPI_SUCCESS) {
+        int resultlen;
+        char err_buffer[MPI_MAX_ERROR_STRING];
+        MPI_Error_string(err,err_buffer,&resultlen);
+        printf("Error:%s | Rank:%d\n",err_buffer,rank);
+        MPI_Abort(MPI_COMM_WORLD,MPI_ERR_IO);
+    }
+}
--- openmpi-1.4.3/ompi/mca/io/romio/romio/adio/ad_plfs/ad_plfs_delete.c	1969-12-31 16:00:00.000000000 -0800
+++ manio-ompi-1.4.3/ompi/mca/io/romio/romio/adio/ad_plfs/ad_plfs_delete.c	2012-11-29 14:17:04.225331098 -0800
@@ -0,0 +1,26 @@
+/* -*- Mode: C; c-basic-offset:4 ; -*- */
+/*
+ *   $Id: ad_plfs_delete.c,v 1.1 2010/11/29 19:59:01 adamm Exp $
+ *
+ *   Copyright (C) 1997 University of Chicago.
+ *   See COPYRIGHT notice in top-level directory.
+ */
+
+#include "ad_plfs.h"
+#include "adio.h"
+
+void ADIOI_PLFS_Delete(char *filename, int *error_code)
+{
+    int err;
+    static char myname[] = "ADIOI_PLFS_DELETE";
+    plfs_debug("%s: begin\n", myname );
+    err = plfs_unlink(filename);
+    if (err < 0) {
+        *error_code = MPIO_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE,
+                                           myname, __LINE__, MPI_ERR_IO,
+                                           "**io",
+                                           "**io %s", strerror(-err));
+    } else {
+        *error_code = MPI_SUCCESS;
+    }
+}
--- openmpi-1.4.3/ompi/mca/io/romio/romio/adio/ad_plfs/ad_plfs_fcntl.c	1969-12-31 16:00:00.000000000 -0800
+++ manio-ompi-1.4.3/ompi/mca/io/romio/romio/adio/ad_plfs/ad_plfs_fcntl.c	2012-11-29 14:17:04.225331098 -0800
@@ -0,0 +1,79 @@
+/* -*- Mode: C; c-basic-offset:4 ; -*- */
+/*
+ *
+ *   Copyright (C) 1997 University of Chicago.
+ *   See COPYRIGHT notice in top-level directory.
+ */
+
+#include "ad_plfs.h"
+
+void ADIOI_PLFS_Fcntl(ADIO_File fd, int flag, ADIO_Fcntl_t *fcntl_struct,
+                      int *error_code)
+{
+    static char myname[] = "ADIOI_PLFS_FCNTL";
+    struct stat buf;
+    int ret, rank, size_only, lazy_stat=1;
+    plfs_debug( "%s: begin\n", myname );
+    switch(flag) {
+    case ADIO_FCNTL_GET_FSIZE:
+        plfs_query(fd->fs_ptr, NULL, NULL, NULL, &lazy_stat);
+        if (lazy_stat == 0) {
+            // every rank calls plfs_sync to flush in-memory index.
+            // this is not a collective operation, take out
+            // all collective calls.
+            plfs_sync(fd->fs_ptr);
+            //plfs_barrier(fd->comm,rank);
+            // rank 0 does slow stat and broadcasts to all.
+            //MPI_Comm_rank(fd->comm, &rank);
+            //if (rank == 0) {
+            size_only = 0;
+            ret = plfs_getattr( fd->fs_ptr, fd->filename, &buf, size_only );
+            //}
+            //MPI_Bcast(&ret, 1, MPI_INT, 0, fd->comm);
+            if (ret == 0) {
+                //MPI_Bcast(&(buf.st_size), 1, MPI_LONG_LONG, 0, fd->comm);
+                fcntl_struct->fsize = buf.st_size;
+                *error_code = MPI_SUCCESS;
+            } else {
+                *error_code = MPIO_Err_create_code(MPI_SUCCESS,
+                                                   MPIR_ERR_RECOVERABLE, myname,
+                                                   __LINE__, MPI_ERR_IO, "**io",
+                                                   "**io %s", strerror(errno));
+            }
+        } else {
+            size_only = 1;  // do lazy stat
+            ret = plfs_getattr( fd->fs_ptr, fd->filename, &buf, size_only );
+            if ( ret == 0 ) {
+                //This is a non collective version of the commented
+                //code below
+                fcntl_struct->fsize = buf.st_size;
+                //These all depend on a collective call
+                //long long tmp_buf;
+                //MPI_Allreduce(&(buf.st_size), &tmp_buf, 1,
+                //              MPI_LONG_LONG, MPI_MAX, fd->comm);
+                //fcntl_struct->fsize = (size_t)tmp_buf;
+                *error_code = MPI_SUCCESS;
+            } else {
+                *error_code = MPIO_Err_create_code(MPI_SUCCESS,
+                                                   MPIR_ERR_RECOVERABLE, myname,
+                                                   __LINE__, MPI_ERR_IO, "**io",
+                                                   "**io %s", strerror(errno));
+            }
+        }
+        //if (fd->fp_sys_posn != -1) {
+        //     pvfs_lseek64(fd->fd_sys, fd->fp_sys_posn, SEEK_SET);
+        //}
+        break;
+    case ADIO_FCNTL_SET_DISKSPACE:
+    case ADIO_FCNTL_SET_ATOMICITY:
+    default:
+        /* --BEGIN ERROR HANDLING-- */
+        *error_code = MPIO_Err_create_code(MPI_SUCCESS,
+                                           MPIR_ERR_RECOVERABLE,
+                                           myname, __LINE__,
+                                           MPI_ERR_ARG,
+                                           "**flag", "**flag %d", flag);
+        return;
+        /* --END ERROR HANDLING-- */
+    }
+}
--- openmpi-1.4.3/ompi/mca/io/romio/romio/adio/ad_plfs/ad_plfs_flush.c	1969-12-31 16:00:00.000000000 -0800
+++ manio-ompi-1.4.3/ompi/mca/io/romio/romio/adio/ad_plfs/ad_plfs_flush.c	2012-11-29 14:17:04.225331098 -0800
@@ -0,0 +1,28 @@
+/* -*- Mode: C; c-basic-offset:4 ; -*- */
+/*
+ *   $Id: ad_plfs_flush.c,v 1.1 2010/11/29 19:59:01 adamm Exp $
+ *
+ *   Copyright (C) 1997 University of Chicago.
+ *   See COPYRIGHT notice in top-level directory.
+ */
+
+#include "ad_plfs.h"
+
+void ADIOI_PLFS_Flush(ADIO_File fd, int *error_code)
+{
+    int err, rank;
+    static char myname[] = "ADIOI_PLFS_FLUSH";
+    plfs_debug( "%s: begin\n", myname );
+    MPI_Comm_rank(fd->comm, &rank);
+    // even though this is a collective routine, everyone must flush here
+    // because everyone has there own data file handle
+    err = plfs_sync(fd->fs_ptr);
+    if (err < 0) {
+        *error_code = MPIO_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE,
+                                           myname, __LINE__, MPI_ERR_IO,
+                                           "**io",
+                                           "**io %s", strerror(-err));
+    } else {
+        *error_code = MPI_SUCCESS;
+    }
+}
--- openmpi-1.4.3/ompi/mca/io/romio/romio/adio/ad_plfs/ad_plfs.h	1969-12-31 16:00:00.000000000 -0800
+++ manio-ompi-1.4.3/ompi/mca/io/romio/romio/adio/ad_plfs/ad_plfs.h	2012-11-29 14:17:04.225331098 -0800
@@ -0,0 +1,82 @@
+/* -*- Mode: C; c-basic-offset:4 ; -*- */
+/*
+ *   $Id: ad_plfs.h,v 1.1 2010/11/29 19:59:01 adamm Exp $
+ *
+ *   Copyright (C) 1997 University of Chicago.
+ *   See COPYRIGHT notice in top-level directory.
+ */
+
+#ifndef AD_PLFS_INCLUDE
+#define AD_PLFS_INCLUDE
+
+#ifndef ROMIOCONF_H_INCLUDED
+#include "romioconf.h"
+#define ROMIOCONF_H_INCLUDED
+#endif
+#ifdef ROMIO_PLFS_NEEDS_INT64_DEFINITION
+typedef long long int int64_t;
+#endif
+
+#include <unistd.h>
+#include <sys/types.h>
+#include <sys/uio.h>
+#include <fcntl.h>
+#include <plfs.h>
+#include <plfs/plfs_internal.h>
+#include "adio.h"
+
+
+void ADIOI_PLFS_Open(ADIO_File fd, int *error_code);
+void ADIOI_PLFS_Close(ADIO_File fd, int *error_code);
+void ADIOI_PLFS_ReadContig(ADIO_File fd, void *buf, int count,
+                           MPI_Datatype datatype, int file_ptr_type,
+                           ADIO_Offset offset, ADIO_Status *status, int
+                           *error_code);
+void ADIOI_PLFS_WriteContig(ADIO_File fd, void *buf, int count,
+                            MPI_Datatype datatype, int file_ptr_type,
+                            ADIO_Offset offset, ADIO_Status *status, int
+                            *error_code);
+void ADIOI_PLFS_Fcntl(ADIO_File fd, int flag, ADIO_Fcntl_t *fcntl_struct, int
+                      *error_code);
+/*
+void ADIOI_PLFS_WriteStrided(ADIO_File fd, void *buf, int count,
+               MPI_Datatype datatype, int file_ptr_type,
+               ADIO_Offset offset, ADIO_Status *status, int
+               *error_code);
+void ADIOI_PLFS_ReadStrided(ADIO_File fd, void *buf, int count,
+               MPI_Datatype datatype, int file_ptr_type,
+               ADIO_Offset offset, ADIO_Status *status, int
+               *error_code);
+void ADIOI_PLFS_SetInfo(ADIO_File fd, MPI_Info users_info, int *error_code);
+*/
+void ADIOI_PLFS_Flush(ADIO_File fd, int *error_code);
+void ADIOI_PLFS_Delete(char *filename, int *error_code);
+void ADIOI_PLFS_Resize(ADIO_File fd, ADIO_Offset size, int *error_code);
+void ADIOI_PLFS_SetInfo(ADIO_File fd, MPI_Info users_info, int *error_code);
+int  ADIOI_PLFS_Feature(ADIO_File fd, int flag);
+
+int plfs_protect_all(const char *file, MPI_Comm comm);
+
+#define plfs_barrier(X,Y) \
+    do { \
+        plfs_debug("Rank %d: Enter barrier from %s:%d", \
+            Y, __FUNCTION__, __LINE__ ); \
+        MPI_Barrier(X); \
+        plfs_debug("Rank %d: Exit barrier from %s:%d", \
+            Y, __FUNCTION__, __LINE__ ); \
+    } while(0);
+    
+
+int ad_plfs_amode( int access_mode );
+void malloc_check(void *test_me,int rank);
+void check_stream(int size,int rank);
+/* Check for hints passed from the command line
+ * Current hints
+ * plfs_enable_broadcast : Turn broadcast of index from root on
+ * plfs_compress_index   : Compress indexes before sending out
+ *                         useless if broadcast off
+ * plfs_flatten_close    : Flatten the index on the close
+ *
+ */
+int ad_plfs_hints(ADIO_File fd, int rank, char *hint);
+#endif
--- openmpi-1.4.3/ompi/mca/io/romio/romio/adio/ad_plfs/ad_plfs_hints.c	1969-12-31 16:00:00.000000000 -0800
+++ manio-ompi-1.4.3/ompi/mca/io/romio/romio/adio/ad_plfs/ad_plfs_hints.c	2012-11-29 14:17:04.225331098 -0800
@@ -0,0 +1,130 @@
+/*
+ *   Copyright 2011 Cray Inc. All Rights Reserved.
+ */
+/*   TODO: other copyrights are probably appropriate. */
+
+#include "ad_plfs.h"
+
+#define POORMANS_GDB \
+    printf("%d in %s:%d\n", rank, __FUNCTION__,__LINE__);
+#ifdef ROMIO_CRAY
+#include "../ad_cray/ad_cray.h"
+#endif /* ROMIO_CRAY */
+
+
+void ADIOI_PLFS_SetInfo(ADIO_File fd, MPI_Info users_info, int *error_code)
+{
+    static char myname[] = "ADIOI_PLFS_SETINFO";
+    char *value;
+    int flag, tmp_val = -1;
+    // The initial values make a difference.
+    int disable_broadcast = 0;
+    int compress_index = 0;
+    int flatten_close = 0;
+    int disable_parindex_read = 0;
+    int gen_error_code,rank;
+    MPI_Comm_rank( fd->comm, &rank );
+    *error_code = MPI_SUCCESS;
+    #ifdef ROMIO_CRAY
+       /* Process any hints set with the MPICH_MPIIO_HINTS
+          environment variable. */
+       ADIOI_CRAY_getenv_mpiio_hints(&users_info, fd);
+    #endif /* ROMIO_CRAY */
+
+    // these optimizations only make sense in container mode
+    if (plfs_get_filetype(fd->filename) != CONTAINER) {
+        disable_broadcast = 1;
+        compress_index = 0;
+        flatten_close = 0;
+        disable_parindex_read = 1;
+    }
+    if ((fd->info) == MPI_INFO_NULL) {
+        /* This must be part of the open call. can set striping parameters
+         * if necessary.
+         */
+        MPI_Info_create(&(fd->info));
+        if (users_info != MPI_INFO_NULL) {
+            value = (char *) ADIOI_Malloc((MPI_MAX_INFO_VAL+1)*sizeof(char));
+            /* plfs_disable_broadcast */
+            MPI_Info_get(users_info, "plfs_disable_broadcast", MPI_MAX_INFO_VAL,
+                         value, &flag);
+            if (flag) {
+                disable_broadcast = atoi(value);
+                tmp_val = disable_broadcast;
+                MPI_Bcast(&tmp_val, 1, MPI_INT, 0, fd->comm);
+                if (tmp_val != disable_broadcast) {
+                    FPRINTF(stderr, "ADIOI_PLFS_SetInfo: "
+                            "the value for key \"plfs_disable_broadcast\" "
+                            "must be the same on all processes\n");
+                    MPI_Abort(MPI_COMM_WORLD, 1);
+                }
+                MPI_Info_set(fd->info, "plfs_disable_broadcast", value);
+            }
+            MPI_Info_get(users_info, "plfs_compress_index", MPI_MAX_INFO_VAL,
+                         value, &flag);
+            /* Compression flag*/
+            if(flag) {
+                compress_index = atoi(value);
+                tmp_val = compress_index;
+                MPI_Bcast(&tmp_val,1,MPI_INT,0,fd->comm);
+                if (tmp_val != compress_index) {
+                    FPRINTF(stderr, "ADIOI_PLFS_SetInfo: "
+                            "the value for key \"plfs_compress_index\" "
+                            "must be the same on all processes\n");
+                    MPI_Abort(MPI_COMM_WORLD, 1);
+                }
+                MPI_Info_set(fd->info, "plfs_compress_index", value);
+            }
+            /* flatten_close */
+            MPI_Info_get(users_info, "plfs_flatten_close", MPI_MAX_INFO_VAL,
+                         value, &flag);
+            if(flag) {
+                flatten_close = atoi(value);
+                tmp_val = flatten_close;
+                MPI_Bcast(&tmp_val,1,MPI_INT,0,fd->comm);
+                if (tmp_val != flatten_close) {
+                    FPRINTF(stderr, "ADIOI_PLFS_SetInfo: "
+                            "the value for key \"plfs_flatten_close\" "
+                            "must be the same on all processes\n");
+                    MPI_Abort(MPI_COMM_WORLD, 1);
+                }
+                MPI_Info_set(fd->info, "plfs_flatten_close", value);
+            }
+            /* Parallel Index Read  */
+            MPI_Info_get(users_info, "plfs_disable_paropen", MPI_MAX_INFO_VAL,
+                         value, &flag);
+            if(flag) {
+                disable_parindex_read = atoi(value);
+                tmp_val = disable_parindex_read;
+                MPI_Bcast(&tmp_val,1,MPI_INT,0,fd->comm);
+                if (tmp_val != disable_parindex_read) {
+                    FPRINTF(stderr, "ADIOI_PLFS_SetInfo: "
+                            "the value for key \"plfs_disable_paropen\" "
+                            "must be the same on all processes\n");
+                    MPI_Abort(MPI_COMM_WORLD, 1);
+                }
+                MPI_Info_set(fd->info, "plfs_disable_paropen", value);
+            }
+            ADIOI_Free(value);
+        }
+    }
+    #ifdef ROMIO_CRAY /* --BEGIN CRAY ADDITION-- */
+        /* Calling the CRAY SetInfo() will add the Cray supported features:
+         * - set the number of aggregators to the number of compute nodes
+         * - MPICH_MPIIO_HINTS environment variable
+         * - MPICH_MPIIO_HINTS_DISPLAY env var to display of hints values
+         * - etc
+         */
+        ADIOI_CRAY_SetInfo(fd, users_info, &gen_error_code); 
+    #else
+        ADIOI_GEN_SetInfo(fd, users_info, &gen_error_code); 
+    #endif /* --END CRAY ADDITION-- */
+
+    /* If this function is successful, use the error code
+     * returned from ADIOI_GEN_SetInfo
+     * otherwise use the error_code generated by this function
+     */
+    if(*error_code == MPI_SUCCESS) {
+        *error_code = gen_error_code;
+    }
+}
--- openmpi-1.4.3/ompi/mca/io/romio/romio/adio/ad_plfs/ad_plfs_open.c	1969-12-31 16:00:00.000000000 -0800
+++ manio-ompi-1.4.3/ompi/mca/io/romio/romio/adio/ad_plfs/ad_plfs_open.c	2012-11-29 14:17:04.225331098 -0800
@@ -0,0 +1,892 @@
+/* -*- Mode: C; c-basic-offset:4 ; -*- */
+/*
+ *   $Id: ad_plfs_open.c,v 1.1 2010/11/29 19:59:01 adamm Exp $
+ *
+ *   Copyright (C) 1997 University of Chicago.
+ *   See COPYRIGHT notice in top-level directory.
+ */
+#include "ad_plfs.h"
+#include "zlib.h"
+#include <dirent.h>
+#include <string.h>
+#include <limits.h>
+#include <assert.h>
+
+
+#define VERBOSE_DEBUG 0
+#define BIT_ARRAY_LENGTH MAX_HOSTDIRS
+
+typedef char Bitmap;
+Bitmap bitmap[BIT_ARRAY_LENGTH/8];
+
+// check whether bit is set in our bitmap
+int
+adplfs_bitIsSet( long n, char *bitmap )
+{
+    long whichByte = n / 8;
+    int  whichBit  = n % 8;
+    return ((bitmap[whichByte] << whichBit) & 0x80) != 0;
+}
+
+// set a bit in a bitmap
+void
+adplfs_setBit( long n, char *bitmap )
+{
+    long whichByte = n / 8;
+    int  whichBit  = n % 8;
+    char temp = bitmap[whichByte];
+    bitmap[whichByte] = (char)(temp | (0x80 >> whichBit));
+}
+
+// clear a bit in a bitmap
+void
+adplfs_clearBit( long n, char *bitmap )
+{
+    long whichByte = n / 8;
+    int  whichBit  = n % 8;
+    char temp = bitmap[whichByte];
+    bitmap[whichByte] = (char)(temp & ~(0x80 >> whichBit));
+}
+
+// A bitmap to hold the number of and id of
+// hostdirs inside of the container
+/*
+typedef struct {
+    unsigned int bit:1;
+}Bit;
+Bit bitmap[BIT_ARRAY_LENGTH]={0};
+*/
+
+// a bunch of helper macros we added when we had a really hard time debugging
+// this file.  We were confused by ADIO calling rank 0 initially on the create
+// and then again on the open (and a bunch of other stuff)
+#if VERBOSE_DEBUG == 1
+#define BITMAP_PRINT adplfs_host_list_print(__LINE__,bitmap);
+
+#define POORMANS_GDB \
+        fprintf(stderr,"%d in %s:%d\n", rank, __FUNCTION__,__LINE__);
+
+#define TEST_BCAST(X) \
+    {\
+        int test = -X;\
+        if(rank==0) { \
+            test = X; \
+        }             \
+        MPIBCAST( &test, 1, MPI_INT, 0, fd->comm );\
+        fprintf(stderr,"rank %d got test %d\n",rank,test);\
+        if(test!=X){ \
+            MPI_Abort(MPI_COMM_WORLD,MPI_ERR_IO);\
+        }\
+    }
+#else
+#define BITMAP_PRINT {}
+#define POORMANS_GDB {}
+#define TEST_BCAST(X) {}
+#endif
+
+#define MPIBCAST(A,B,C,D,E) \
+    POORMANS_GDB \
+    { \
+        ret = MPI_Bcast(A,B,C,D,E); \
+        if(ret!=MPI_SUCCESS) { \
+            int resultlen; \
+            char err_buffer[MPI_MAX_ERROR_STRING]; \
+            MPI_Error_string(ret,err_buffer,&resultlen); \
+            printf("Error:%s | Rank:%d\n",err_buffer,rank); \
+            MPI_Abort(MPI_COMM_WORLD,MPI_ERR_IO); \
+        } \
+    } \
+    POORMANS_GDB
+
+#define MPIALLGATHER(A,B,C,D,E,F,G)\
+{\
+    ret = MPI_Allgather(A,B,C,D,E,F,G);\
+    if(ret!= MPI_SUCCESS){\
+        int resultlen; \
+        char err_buffer[MPI_MAX_ERROR_STRING]; \
+        MPI_Error_string(ret,err_buffer,&resultlen); \
+        printf("Error:%s | Rank:%d\n",err_buffer,rank); \
+        MPI_Abort(MPI_COMM_WORLD,MPI_ERR_IO); \
+    } \
+} \
+ 
+#define MPIALLGATHERV(A,B,C,D,E,F,G,H)\
+{\
+    ret = MPI_Allgatherv(A,B,C,D,E,F,G,H);\
+    if(ret!= MPI_SUCCESS){\
+        int resultlen; \
+        char err_buffer[MPI_MAX_ERROR_STRING]; \
+        MPI_Error_string(ret,err_buffer,&resultlen); \
+        printf("Error:%s | Rank:%d\n",err_buffer,rank); \
+        MPI_Abort(MPI_COMM_WORLD,MPI_ERR_IO); \
+    } \
+}
+
+#define MPIGATHER(A,B,C,D,E,F,G,H)\
+{\
+    ret = MPI_Gather(A,B,C,D,E,F,G,H);\
+    if(ret!= MPI_SUCCESS){\
+        int resultlen; \
+        char err_buffer[MPI_MAX_ERROR_STRING]; \
+        MPI_Error_string(ret,err_buffer,&resultlen); \
+        printf("Error:%s | Rank:%d\n",err_buffer,rank); \
+        MPI_Abort(MPI_COMM_WORLD,MPI_ERR_IO); \
+    } \
+}
+
+#define MPIGATHERV(A,B,C,D,E,F,G,H,I)\
+{\
+    ret = MPI_Gatherv(A,B,C,D,E,F,G,H,I);\
+    if(ret!= MPI_SUCCESS){\
+        int resultlen; \
+        char err_buffer[MPI_MAX_ERROR_STRING]; \
+        MPI_Error_string(ret,err_buffer,&resultlen); \
+        printf("Error:%s | Rank:%d\n",err_buffer,rank); \
+        MPI_Abort(MPI_COMM_WORLD,MPI_ERR_IO); \
+    } \
+}
+
+
+int adplfs_open_helper(ADIO_File fd,Plfs_fd **pfd,int *error_code,int perm,
+                int amode,int rank);
+int adplfs_broadcast_index(Plfs_fd **pfd, ADIO_File fd,
+                    int *error_code,int perm,int amode,int rank,
+                    int compress_flag);
+int adplfs_getPerm(ADIO_File);
+
+int adplfs_getPerm(ADIO_File fd)
+{
+    int perm = fd->perm;
+    if (fd->perm == ADIO_PERM_NULL) {
+        int old_mask = umask(022);
+        umask(old_mask);
+        perm = old_mask ^ 0666;
+    }
+    return perm;
+}
+// Par index read stuff
+int adplfs_par_index_read(ADIO_File fd,Plfs_fd **pfd,int *error_code,int perm,
+                   int amode,int rank, void **global_index);
+// Fills in the bitmap structure
+int adplfs_num_host_dirs(int *hostdir_count,char *target);
+// Printer for the bitmap struct
+void adplfs_host_list_print(int line, Bitmap *bitmap);
+// Function to calculate the extra ranks
+int adplfs_extra_rank_calc(int np,int num_host_dir);
+// Number of ranks per comm
+int adplfs_ranks_per_comm_calc(int np,int num_host_dir);
+// Based on my rank how many ranks are in my hostdir comm
+int adplfs_rank_to_size(int rank,int ranks_per_comm,int extra_rank,
+                 int np,int group_index);
+// Index used for a color that determines my hostdir comm
+int adplfs_rank_to_group_index(int rank,int ranks_per_comm,int extra_rank);
+// Converts bitmap position to a dirname
+char *adplfs_bitmap_to_dirname(Bitmap *bitmap,int group_index,
+                        char *target,int mult,int np);
+// Called when num procs >= num_host_dirs
+void adplfs_split_and_merge(ADIO_File fd,int rank,int extra_rank,
+                     int ranks_per_comm,int np,char *filename,
+                     void **global_index);
+// Called when num hostdirs > num procs
+void adplfs_read_and_merge(ADIO_File fd,int rank,
+                    int np,int hostdir_per_rank,char *filename,
+                    void **global_index);
+// Added to handle the case where one rank must read more than one hostdir
+char *adplfs_count_to_hostdir(Bitmap *bitmap,int stop_point,int *count,
+                       int *hostdir_found,char *filename,char *target,
+                       int first);
+// Broadcast the bitmap to interested parties
+void adplfs_bcast_bitmap(MPI_Comm comm,int rank);
+
+void ADIOI_PLFS_Open(ADIO_File fd, int *error_code)
+{
+    Plfs_fd *pfd =NULL;
+    // I think perm is the mode and amode is the flags
+    int err = 0,perm, amode, old_mask,rank,ret;
+    MPI_Comm_rank( fd->comm, &rank );
+    static char myname[] = "ADIOI_PLFS_OPEN";
+    perm = adplfs_getPerm(fd);
+    amode = ad_plfs_amode(fd->access_mode);
+    // ADIO makes 2 calls into here:
+    // first, just 0 with CREATE
+    // then everyone without
+    if (fd->access_mode & ADIO_CREATE) {
+        err = plfs_create(fd->filename, perm, amode, rank);
+        if ( err != 0 ) {
+            *error_code =MPIO_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE,
+                                              myname, __LINE__, MPI_ERR_IO,
+                                              "**io",
+                                              "**io %s", strerror(-err));
+            errno = -err;
+        } else {
+            *error_code = MPI_SUCCESS;
+        }
+        fd->fs_ptr = NULL; // set null because ADIO is about to close it
+        return;
+    }
+    // if we make it here, we're doing RDONLY, WRONLY, or RDWR
+    // at this point, we want to do different for container/flat_file mode
+    if (plfs_get_filetype(fd->filename) != CONTAINER) {
+        err = plfs_open(&pfd,fd->filename,amode,rank,perm,NULL);
+        if ( err < 0 ) {
+            *error_code = MPIO_Err_create_code(MPI_SUCCESS,
+                                               MPIR_ERR_RECOVERABLE,
+                                               myname, __LINE__, MPI_ERR_IO,
+                                               "**io",
+                                               "**io %s", strerror(-err));
+            plfs_debug( "%s: failure %s\n", myname, strerror(-err) );
+            return;
+        } else {
+            plfs_debug( "%s: Success on open(%d)!\n", myname, rank );
+            fd->fs_ptr = pfd;
+            fd->fd_direct = -1;
+            *error_code = MPI_SUCCESS;
+            return;
+        }
+    }
+    // if we get here, we're in container mode; continue with the optimizations
+    ret = adplfs_open_helper(fd,&pfd,error_code,perm,amode,rank);
+    MPI_Allreduce(&ret, &err, 1, MPI_INT, MPI_MIN, fd->comm);
+    if ( err != 0 ) {
+        *error_code = MPIO_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE,
+                                           myname, __LINE__, MPI_ERR_IO,
+                                           "**io",
+                                           "**io %s", strerror(-err));
+        errno = -err;
+        plfs_debug( "%s: failure %s\n", myname, strerror(-err) );
+        return;
+    } else {
+        plfs_debug( "%s: Success on open (%d)!\n", myname, rank );
+        *error_code = MPI_SUCCESS;
+    }
+    return;
+}
+
+// a helper that determines whether 0 distributes the index to everyone else
+// or whether everyone just calls plfs_open directly
+int adplfs_open_helper(ADIO_File fd,Plfs_fd **pfd,int *error_code,int perm,
+                int amode,int rank)
+{
+    int err = 0, disabl_broadcast=0, compress_flag=0,close_flatten=0;
+    int parallel_index_read=1;
+    static char myname[] = "ADIOI_PLFS_OPENHELPER";
+    Plfs_open_opt open_opt;
+    MPI_Comm hostdir_comm;
+    int hostdir_rank, write_mode;
+    open_opt.reopen = 0;
+    // get a hostdir comm to use to serialize write a bit
+    write_mode = (fd->access_mode==ADIO_RDONLY?0:1);
+    if (write_mode) {
+        size_t color = plfs_gethostdir_id(plfs_gethostname());
+        err = MPI_Comm_split(fd->comm,color,rank,&hostdir_comm);
+        if(err!=MPI_SUCCESS) {
+            return err;
+        }
+        MPI_Comm_rank(hostdir_comm,&hostdir_rank);
+    }
+    // get specified behavior from hints
+    if (fd->access_mode==ADIO_RDONLY) {
+        disabl_broadcast = ad_plfs_hints(fd,rank,"plfs_disable_broadcast");
+        compress_flag = ad_plfs_hints(fd,rank,"plfs_compress_index");
+        parallel_index_read =!ad_plfs_hints(fd,rank,"plfs_disable_paropen");
+        plfs_debug("Disable_bcast:%d,compress_flag:%d,parindex:%d\n",
+                   disabl_broadcast,compress_flag,parallel_index_read);
+        // I took out the extra broadcasts at this point. ad_plfs_hints
+        // has code to make sure that all ranks have the same value
+        // for the hint
+    } else {
+        disabl_broadcast = 1; // don't create an index unless we're in read mode
+        compress_flag=0;
+    }
+    // This is new code added to handle the parallel_index_read case
+    if( fd->access_mode==ADIO_RDONLY && parallel_index_read) {
+        void *global_index;
+        // Function to start the parallel index read
+        err = adplfs_par_index_read(fd,pfd,error_code,perm,amode,rank,
+                             &global_index);
+        if (err == 0) {
+            open_opt.pinter = PLFS_MPIIO;
+            open_opt.index_stream=global_index;
+            err = plfs_open(pfd,fd->filename,amode,rank,perm,&open_opt);
+            free(global_index);
+        }
+    } else if(fd->access_mode==ADIO_RDONLY && !disabl_broadcast) {
+        // If we are RDONLY and broadcast isn't disabled let's broadcast it
+        err = adplfs_broadcast_index(pfd,fd,error_code,perm,amode,rank,compress_flag);
+    } else {
+        // here we are either writing or reading without optimizations
+        open_opt.pinter = PLFS_MPIIO;
+        open_opt.index_stream=NULL;
+        close_flatten = ad_plfs_hints(fd,rank,"plfs_flatten_close");
+        // Let's only buffer when the flatten on close hint is passed
+        // and we are in WRONLY mode
+        open_opt.buffer_index=close_flatten;
+        plfs_debug("Opening without a broadcast\n");
+        // everyone opens themselves (write mode or independent read mode)
+        // hostdir_rank zeros do the open first on the write
+        if (write_mode && hostdir_rank) {
+            plfs_barrier(hostdir_comm,rank);
+        }
+        err = plfs_open( pfd, fd->filename, amode, rank, perm ,&open_opt);
+        if (write_mode && !hostdir_rank) {
+            plfs_barrier(hostdir_comm,rank);
+        }
+    }
+    // clean up the communicator we used
+    if (write_mode) {
+        MPI_Comm_free(&hostdir_comm);
+    }
+    if ( err < 0 ) {
+        *error_code = MPIO_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE,
+                                           myname, __LINE__, MPI_ERR_IO,
+                                           "**io",
+                                           "**io %s", strerror(-err));
+        plfs_debug( "%s: failure %s\n", myname, strerror(-err) );
+        errno = -err;
+        return err;
+    } else {
+        plfs_debug( "%s: Success on open(%d)!\n", myname, rank );
+        fd->fs_ptr = *pfd;
+        fd->fd_direct = -1;
+        *error_code = MPI_SUCCESS;
+        return 0;
+    }
+}
+
+// 0 gets the index by calling plfs_open() first and then extracting the index
+// it then broadcasts that to the rest who then pass it to their own plfs_open()
+int adplfs_broadcast_index(Plfs_fd **pfd, ADIO_File fd,
+                    int *error_code,int perm,int amode,int rank,
+                    int compress_flag)
+{
+    int err = 0,ret;
+    char *index_stream;
+    char *compr_index;
+    // [0] is index stream size [1] is compressed size
+    unsigned long index_size[2]= {0};
+    Plfs_open_opt open_opt;
+    open_opt.pinter = PLFS_MPIIO;
+    open_opt.index_stream=NULL;
+    open_opt.buffer_index=0;
+    open_opt.reopen = 0;
+    if(rank==0) {
+        err = plfs_open(pfd, fd->filename, amode, rank, perm , &open_opt);
+    }
+    MPIBCAST(&err,1,MPI_INT,0,fd->comm);   // was 0's open successful?
+    if(err !=0 ) {
+        return err;
+    }
+    // rank 0 turns the index into a stream, broadcasts its size, then it
+    if(rank==0) {
+        plfs_debug("In broadcast index with compress_flag:%d\n",compress_flag);
+        index_size[0] = index_size[1] = plfs_index_stream(pfd,&index_stream);
+        if(index_size[0]<0) {
+            MPI_Abort(MPI_COMM_WORLD,MPI_ERR_IO);
+        }
+        if(compress_flag) {
+            plfs_debug("About to malloc the compressed index space\n");
+            compr_index=malloc(index_size[0]);
+            // Check the malloc
+            if(!compr_index) {
+                plfs_debug("Rank %d aborting because of failed malloc\n",rank);
+                MPI_Abort(MPI_COMM_WORLD,MPI_ERR_IO);
+            }
+            plfs_debug("About to compress the index\n");
+            // Check the compress
+            if(plfs_compress(compr_index,&index_size[1],index_stream,index_size[0])
+                    !=Z_OK) {
+                plfs_debug("Compression of index has failed\n");
+                MPI_Abort(MPI_COMM_WORLD,MPI_ERR_IO);
+            }
+        }
+    }
+    // Original index stream size
+    if (!rank) {
+        plfs_debug("Broadcasting the sizes of the index:%d "
+                   "and compressed index%d\n" ,index_size[0],index_size[1]);
+    }
+    MPIBCAST(index_size, 2, MPI_LONG, 0, fd->comm);
+    if(rank!=0) {
+        index_stream = malloc(index_size[0]);
+        if(compress_flag) {
+            compr_index = malloc(index_size[1]);
+            if(!compr_index ) {
+                plfs_debug("Rank %d aborting because of failed malloc\n",rank);
+                MPI_Abort(MPI_COMM_WORLD,MPI_ERR_IO);
+            }
+        }
+        //We need to check that the malloc succeeded or the broadcast is in vain
+        if(!index_stream ) {
+            plfs_debug("Rank %d aborting because of a failed malloc\n",rank);
+            MPI_Abort(MPI_COMM_WORLD,MPI_ERR_IO);
+        }
+    }
+    if (compress_flag) {
+        if(!rank) {
+            plfs_debug("Broadcasting compressed index\n");
+        }
+        MPIBCAST(compr_index,index_size[1],MPI_CHAR,0,fd->comm);
+    } else {
+        if(!rank) {
+            plfs_debug("Broadcasting full index\n");
+        }
+        MPIBCAST(index_stream,index_size[0],MPI_CHAR,0,fd->comm);
+    }
+    // Broadcast compressed index
+    if(rank!=0) {
+        unsigned long uncompr_len=index_size[0];
+        // Uncompress the index
+        if(compress_flag) {
+            plfs_debug("Rank: %d has compr_len %d and expected expanded of %d\n"
+                       ,rank,index_size[1],uncompr_len);
+            int ret=plfs_uncompress(index_stream,
+                               &uncompr_len,compr_index,index_size[1]);
+            if(ret!=Z_OK) {
+                plfs_debug("Rank %d aborting bec failed uncompress\n",rank);
+                if(ret==Z_MEM_ERROR) {
+                    plfs_debug("Mem error\n");
+                }
+                if(ret==Z_BUF_ERROR) {
+                    plfs_debug("Buffer error\n");
+                }
+                if(ret==Z_DATA_ERROR) {
+                    plfs_debug("Data error\n");
+                }
+                MPI_Abort(MPI_COMM_WORLD,MPI_ERR_IO);
+            }
+            // Error if the uncompressed length doesn't match original length
+            if(uncompr_len!=index_size[0]) {
+                plfs_debug("Uncompressed len != original index size\n");
+                MPI_Abort(MPI_COMM_WORLD,MPI_ERR_IO);
+            }
+        }
+        open_opt.index_stream=index_stream;
+        err = plfs_open(pfd,fd->filename,amode,rank,perm,&open_opt);
+    }
+    if(compress_flag) {
+        free(compr_index);
+    }
+    free(index_stream);
+    return 0;
+}
+
+// returns 0 or -errno
+int adplfs_par_index_read(ADIO_File fd,Plfs_fd **pfd,int *error_code,int perm,
+                   int amode,int rank, void **global_index)
+{
+    // Each rank and the number of processes playing
+    int np,extra_rank,ret;
+    MPI_Comm_size(fd->comm, &np);
+    // Rank 0 reads the top level directory and sets the
+    // next two variables
+    int num_host_dir=0;
+    // Every other rank can figures this out using num_host_dir
+    int ranks_per_comm=0;
+    // Only used is ranks per comm equals zero
+    int hostdir_per_rank=0;
+    char *filename;
+    plfs_expand_path(fd->filename,&filename);
+    // Rank 0 only code
+    int bitmap_bcast_sz = (BIT_ARRAY_LENGTH/8)/sizeof(MPI_CHAR);
+    memset(bitmap,0,bitmap_bcast_sz);
+    if(!rank) {
+        // Find out how many hostdirs we currently have
+        // and save info in a bitmap
+        adplfs_num_host_dirs(&num_host_dir,filename);
+        plfs_debug("Num of hostdirs calculated is |%d|\n",num_host_dir);
+    }
+    // Bcast usage brought down by the MPI_Comm_split
+    // Was using MPI_Group_incl which needed an array of
+    // all members of the group. Don't forget to plan and
+    // look at available methods to get the job done
+    MPIBCAST(&num_host_dir,1,MPI_INT,0,fd->comm);
+    if(num_host_dir < 0) {
+        return num_host_dir;
+    }
+    BITMAP_PRINT;
+    // Get some information used to determine the group index
+    ranks_per_comm=adplfs_ranks_per_comm_calc(np,num_host_dir);
+    plfs_debug("%d ranks per comm\n", ranks_per_comm);
+    // Split based on the number of ranks per comm. If zero we
+    // take another path and the extra_rank and hostdir_per_rank
+    // calculation are different
+    if(ranks_per_comm) {
+        extra_rank=adplfs_extra_rank_calc(np,num_host_dir);
+    }
+    if(!ranks_per_comm) {
+        extra_rank=num_host_dir-np;
+        hostdir_per_rank=num_host_dir/np;
+        int left_over=num_host_dir%np;
+        if(rank<left_over) {
+            hostdir_per_rank++;
+        }
+    }
+    plfs_debug("ranks_per_comm=%d,hostdir_per_rank=%d\n",ranks_per_comm,
+               hostdir_per_rank);
+    // Here we split on the ranks per comm. Should not be necessary
+    // to check return values. Functions will abort if an error is encountered
+    if(ranks_per_comm) {
+        adplfs_split_and_merge(fd,rank,extra_rank,
+                        ranks_per_comm,np,filename,
+                        global_index);
+    }
+    if(!ranks_per_comm) {
+        adplfs_read_and_merge(fd,rank,np,hostdir_per_rank,
+                       filename,global_index);
+    }
+    if(filename!=NULL) {
+        free(filename);
+    }
+    return 0;
+}
+
+// This is the case where a rank has to read more than one hostdir
+void adplfs_read_and_merge(ADIO_File fd,int rank,
+                    int np,int hostdir_per_rank,char *filename,
+                    void **global_index)
+{
+    char *targets;
+    int index_sz,ret,count,*index_sizes,*index_disp,index_total_size=0;
+    int global_index_sz;
+    void *index_stream,*index_streams;
+    // Get the bitmap
+    adplfs_bcast_bitmap(fd->comm,rank);
+    // Figure out which hostdirs I have to read
+    targets=adplfs_bitmap_to_dirname(bitmap,rank,filename,
+                              hostdir_per_rank,np);
+    // Read the hostdirs and return an index stream
+    index_sz=plfs_hostdir_rddir(&index_stream,targets,rank,filename);
+    // Make sure it was malloced
+    check_stream(index_sz,rank);
+    // Targets no longer needed
+    free(targets);
+    // Used to hold the sizes of indexes from all procs
+    // needed for ALLGATHERV
+    index_sizes=malloc(sizeof(int)*np);
+    malloc_check((void *)index_sizes,rank);
+    // Gets the index sizes from all ranks
+    MPIALLGATHER(&index_sz,1,MPI_INT,index_sizes,
+                 1,MPI_INT,fd->comm);
+    // Set up displacements
+    index_disp=malloc(sizeof(int)*np);
+    malloc_check((void *)index_disp,rank);
+    for(count=0; count<np; count++) {
+        index_disp[count]=index_total_size;
+        index_total_size+=index_sizes[count];
+    }
+    // Holds all of the index streams from all procs involved in the process
+    index_streams=malloc(sizeof(char)*index_total_size);
+    malloc_check(index_streams,rank);
+    // ALLGATHERV grabs all of the indexes from all the procs
+    MPIALLGATHERV(index_stream,index_sz,MPI_CHAR,index_streams,index_sizes,
+                  index_disp,MPI_CHAR,fd->comm);
+    // Merge all of the stream that we were passed to get a global index
+    global_index_sz=plfs_parindexread_merge(filename,index_streams,
+                                            index_sizes,np,global_index);
+    check_stream(global_index_sz,rank);
+    plfs_debug("Rank |%d| global size |%d|\n",rank,global_index_sz);
+    free(index_streams);
+}
+
+void adplfs_bcast_bitmap(MPI_Comm comm, int rank)
+{
+    int ret;
+    int bitmap_bcast_sz = (BIT_ARRAY_LENGTH/8)/sizeof(MPI_UNSIGNED_CHAR);
+    BITMAP_PRINT;
+    MPIBCAST(bitmap,bitmap_bcast_sz,MPI_UNSIGNED_CHAR,0,comm);
+    BITMAP_PRINT;
+}
+
+// If ranks > hostdirs we can split up our comm
+void adplfs_split_and_merge(ADIO_File fd,int rank,int extra_rank,
+                     int ranks_per_comm,int np,char *filename,
+                     void **global_index)
+{
+    int new_rank,color,group_index,hc_sz,ret,buf_sz=0;
+    int *index_sizes,*index_disp,count,index_total_size=0;
+    char *index_files, *index_streams;
+    void *index_stream;
+    MPI_Comm hostdir_comm,hostdir_zeros_comm;
+    // Group index is the color
+    group_index = adplfs_rank_to_group_index(rank,ranks_per_comm,extra_rank);
+    // Split the world communicator
+    MPI_Comm_split(fd->comm,group_index,rank,&hostdir_comm);
+    MPI_Comm_size(hostdir_comm,&hc_sz);
+    // Grab our rank within the hostdir communicator
+    MPI_Comm_rank (hostdir_comm, &new_rank);
+    // Get a color for a communicator between all rank 0's in a hostdir comm
+    if(!new_rank) {
+        color = 1;
+    } else {
+        color = MPI_UNDEFINED;
+    }
+    // The split for hostdir zeros comm
+    MPI_Comm_split(fd->comm,color,rank,&hostdir_zeros_comm);
+    // Hostdir zeros
+    if(!new_rank) {
+        char *subdir;
+        // Broadcast the bitmap to the leaders
+        BITMAP_PRINT;
+        adplfs_bcast_bitmap(hostdir_zeros_comm,rank);
+        BITMAP_PRINT;
+        // Convert my group index into the dir I should read
+        subdir= adplfs_bitmap_to_dirname(bitmap,group_index,filename,0,np);
+        // Hostdir zero reads the hostdir and converts into a list
+        buf_sz=plfs_hostdir_zero_rddir((void **)&index_files,subdir,rank);
+        check_stream(buf_sz,rank);
+        free(subdir);
+    }
+    // Send the size of the hostdir file list
+    MPIBCAST(&buf_sz,1,MPI_INT,0,hostdir_comm);
+    // Get space for the hostdir file list
+    if(new_rank) {
+        index_files=malloc(sizeof(MPI_CHAR)*buf_sz);
+        malloc_check(index_files,rank);
+    }
+    // Get the hostdir file list
+    MPIBCAST(index_files,buf_sz,MPI_CHAR,0,hostdir_comm);
+    // Take the hostdir file list and convert to an index stream
+    buf_sz=plfs_parindex_read(new_rank,hc_sz,index_files,&index_stream,
+                              filename);
+    check_stream(buf_sz,rank);
+    free(index_files);
+    if(!new_rank) {
+        index_sizes=malloc(sizeof(int)*hc_sz);
+        malloc_check((void *)index_sizes,rank);
+    }
+    // Make sure hostdir rank 0 knows how much index data to expect
+    MPIGATHER(&buf_sz,1,MPI_INT,index_sizes,1,
+              MPI_INT,0,hostdir_comm);
+    // Set up displacements
+    if(!new_rank) {
+        index_disp=malloc(sizeof(int)*hc_sz);
+        malloc_check((void *)index_disp,rank);
+        for(count=0; count<hc_sz; count++) {
+            // Displacements
+            index_disp[count]=index_total_size;
+            index_total_size+=index_sizes[count];
+        }
+        // Space for the index streams from my hostdir comm
+        index_streams=malloc(sizeof(char)*index_total_size);
+        malloc_check(index_stream,rank);
+    }
+    // Gather the index streams from your hostdir
+    MPIGATHERV(index_stream,buf_sz,MPI_CHAR,index_streams,index_sizes,
+               index_disp,MPI_CHAR,0,hostdir_comm);
+    void *hostdir_index_stream;
+    int hostdir_index_sz;
+    // Hostdir leader
+    if(!new_rank) {
+        // Merge the indexes that were gathered
+        hostdir_index_sz=plfs_parindexread_merge(filename,index_streams,
+                         index_sizes,hc_sz,&hostdir_index_stream);
+        free(index_disp);
+        free(index_sizes);
+    }
+    // No longer need this information
+    free(index_stream);
+    int hzc_size,global_index_sz;
+    // Hostdir leader pass information to the other leaders
+    if(!new_rank) {
+        index_total_size=0;
+        MPI_Comm_size(hostdir_zeros_comm,&hzc_size);
+        // Store the sizes of all the leaders index streams
+        index_sizes=malloc(sizeof(int)*hzc_size);
+        malloc_check((void *)index_sizes,rank);
+        // Grab these sizes
+        MPIALLGATHER(&hostdir_index_sz,1,MPI_INT,index_sizes,
+                     1,MPI_INT,hostdir_zeros_comm);
+        // Set up for GatherV
+        index_disp=malloc(sizeof(int)*hzc_size);
+        malloc_check((void *) index_disp,rank);
+        for(count=0; count<hzc_size; count++) {
+            // Displacements
+            index_disp[count]=index_total_size;
+            index_total_size+=index_sizes[count];
+        }
+        index_streams=malloc(sizeof(char)*index_total_size);
+        malloc_check(index_streams,rank);
+        // Receive the index streams from all hotdir zeros
+        MPIALLGATHERV(hostdir_index_stream,hostdir_index_sz,MPI_CHAR,
+                      index_streams,index_sizes,index_disp,MPI_CHAR,
+                      hostdir_zeros_comm);
+        // Merge these streams into a single global index
+        global_index_sz=plfs_parindexread_merge(filename,index_streams,
+                                                index_sizes,hzc_size,
+                                                global_index);
+        check_stream(global_index_sz,rank);
+        // Free all of our malloced structures
+        free(index_streams);
+        free(hostdir_index_stream);
+        free(index_disp);
+        free(index_sizes);
+    }
+    // Get the size of the global index
+    MPIBCAST(&global_index_sz,1,MPI_INT,0,hostdir_comm);
+    // Malloc space if we are not hostdir leaders
+    if(new_rank) {
+        *global_index=malloc(sizeof(char)*global_index_sz);
+    }
+    malloc_check(global_index,rank);
+    //Hostdir leaders broadcast the global index
+    MPIBCAST(*global_index,global_index_sz,MPI_CHAR,0,hostdir_comm);
+    // Don't forget to  call Comm free  on created comms before MPI_Finalize
+    // or you will encounter problems
+    MPI_Comm_free(&hostdir_comm);
+    if(!new_rank) {
+        MPI_Comm_free(&hostdir_zeros_comm);
+    }
+}
+
+// Simple print function
+void adplfs_host_list_print(int line, Bitmap *bitmap)
+{
+    int count;
+    plfs_debug("printing hostdir bitmap from %d:\n", line);
+    for(count=0; count<BIT_ARRAY_LENGTH; count++) {
+        if(adplfs_bitIsSet(count,bitmap)) {
+            plfs_debug("Hostdir at position %d\n",count);
+        }
+    }
+}
+
+// Function that reads in the hostdirs and sets the bitmap
+// this function still works even with metalink stuff
+// probably though we should make an opaque function in
+// Container.cpp that encapsulates this....
+// returns -errno if the opendir fails
+// returns -EISDIR if it's actually a directory and not a file
+// returns a positive number otherwise as even an empty container
+// will have at least one hostdir
+// hmmm.  this function does a readdir.  be nice to move this into
+// library and use new readdirop class
+int
+adplfs_num_host_dirs(int *hostdir_count,char *target)
+{
+    // Directory reading variables
+    DIR *dirp;
+    struct dirent *dirent;
+    int isfile = 0;
+    *hostdir_count = 0;
+    // Open the directory and check value
+    if((dirp=opendir(target)) == NULL) {
+        plfs_debug("Num hostdir opendir error on %s\n",target);
+        *hostdir_count = -errno;
+        return *hostdir_count;
+    }
+    // Start reading the directory
+    while(dirent = readdir(dirp) ) {
+        // Look for entries that beging with hostdir
+        if(strncmp(HOSTDIRPREFIX,dirent->d_name,strlen(HOSTDIRPREFIX))==0) {
+            char *substr;
+            substr=strtok(dirent->d_name,".");
+            substr=strtok(NULL,".");
+            int index = atoi(substr);
+            if (index>=MAX_HOSTDIRS) {
+                fprintf(stderr,"Bad behavior in PLFS.  Too many subdirs.\n");
+                *hostdir_count = -ENOSYS;
+                return *hostdir_count;
+            }
+            plfs_debug("Added a hostdir for %d\n", index);
+            (*hostdir_count)++;
+            adplfs_setBit(index,bitmap);
+        } else if (strncmp(ACCESSFILE,dirent->d_name,strlen(ACCESSFILE))==0) {
+            isfile = 1;
+        }
+    }
+    // Close the dir error out if we have a problem
+    if (closedir(dirp) == -1 ) {
+        plfs_debug("Num hostdir closedir error on %s\n",target);
+        *hostdir_count = -errno;
+        return *hostdir_count;
+    }
+    BITMAP_PRINT;
+    plfs_debug("%s of %s isfile %d hostdirs %d\n",
+               __FUNCTION__,target,isfile,*hostdir_count);
+    if (!isfile) {
+        *hostdir_count = -EISDIR;
+    }
+    return *hostdir_count;
+}
+
+// Calculates the number of ranks per communication group
+// Split comm makes this many subgroups
+int adplfs_ranks_per_comm_calc(int np,int num_host_dir)
+{
+    return (num_host_dir>0?np/num_host_dir:0);
+}
+
+// Get the amount of left over ranks, our values
+// are not going to divide evenly
+int adplfs_extra_rank_calc(int np,int num_host_dir)
+{
+    return  np%num_host_dir;
+}
+
+// Using the rank get the index/color that determines the
+// subcommunication group that we belong in
+int adplfs_rank_to_group_index(int rank,int ranks_per_comm,int extra_rank)
+{
+    int ret;
+    if (rank < (extra_rank*(ranks_per_comm+1))) {
+        ret = rank / (ranks_per_comm+1);
+    } else {
+        ret = (rank - extra_rank)/ranks_per_comm;
+    }
+    return ret;
+}
+
+
+char *adplfs_count_to_hostdir(Bitmap *bitmap,int stop_point,int *count,
+                       int *hostdir_found, char *filename,char *target,
+                       int first)
+{
+    char hostdir_num[16];
+    plfs_debug("Searching bitmap from %d to %d\n",*count,stop_point);
+    while((*hostdir_found)<stop_point) {
+        if(adplfs_bitIsSet(*count,bitmap)) {
+            (*hostdir_found)++;
+        }
+        (*count)++;
+    }
+    if(!first) {
+        strcpy(filename,target);
+    }
+    if(first) {
+        strcat(filename,target);
+    }
+    strcat(filename,"/");
+    strcat(filename,HOSTDIRPREFIX);
+    sprintf(hostdir_num,"%d",(*count)-1);
+    strcat(filename,hostdir_num);
+    return filename;
+}
+
+char *adplfs_bitmap_to_dirname(Bitmap *bitmap,int group_index,
+                        char *target,int mult,int np)
+{
+    char *path;
+    int count = 0;
+    int hostdir_found=0;
+    BITMAP_PRINT;
+    if(mult==0) {
+        path=malloc(sizeof(char)*4096);
+        path=adplfs_count_to_hostdir(bitmap,group_index+1,&count,
+                              &hostdir_found,path,target,0);
+    } else {
+        path=malloc(sizeof(char)*(mult*4096));
+        int dirs=0;
+        while(dirs<mult) {
+            int stop_point;
+            stop_point=(group_index+1)+(dirs*np);
+            path = adplfs_count_to_hostdir(bitmap,stop_point,&count,
+                                    &hostdir_found,path,target,count);
+            strcat(path,"|");
+            dirs++;
+        }
+    }
+    BITMAP_PRINT;
+    plfs_debug("%s returning %s (mult %d)\n", __FUNCTION__, path,mult);
+    return path;
+}
+
--- openmpi-1.4.3/ompi/mca/io/romio/romio/adio/ad_plfs/ad_plfs_read.c	1969-12-31 16:00:00.000000000 -0800
+++ manio-ompi-1.4.3/ompi/mca/io/romio/romio/adio/ad_plfs/ad_plfs_read.c	2012-11-29 14:17:04.225331098 -0800
@@ -0,0 +1,74 @@
+/* -*- Mode: C; c-basic-offset:4 ; -*- */
+/*
+ *   $Id: ad_plfs_read.c,v 1.1 2010/11/29 19:59:01 adamm Exp $
+ *
+ *   Copyright (C) 1997 University of Chicago.
+ *   See COPYRIGHT notice in top-level directory.
+ */
+
+#include "adio.h"
+#include "adio_extern.h"
+#include "ad_plfs.h"
+
+#ifdef ROMIO_CRAY
+#include "../ad_cray/ad_cray.h"
+#endif /* ROMIO_CRAY */
+
+void ADIOI_PLFS_ReadContig(ADIO_File fd, void *buf, int count,
+                           MPI_Datatype datatype, int file_ptr_type,
+                           ADIO_Offset offset, ADIO_Status *status,
+                           int *error_code)
+{
+    int err=-1, datatype_size, rank;
+    ADIO_Offset len;
+    ADIO_Offset myoff;
+    static char myname[] = "ADIOI_PLFS_READCONTIG";
+#ifdef ROMIO_CRAY
+MPIIO_TIMER_START(RSYSIO);
+#endif /* ROMIO_CRAY */
+    MPI_Type_size(datatype, &datatype_size);
+    len = (ADIO_Offset)datatype_size * (ADIO_Offset)count;
+    MPI_Comm_rank( fd->comm, &rank );
+    // for the romio/test/large_file we always get an offset of 0
+    // maybe we need to increment fd->fp_ind ourselves?
+    if (file_ptr_type == ADIO_EXPLICIT_OFFSET) {
+        myoff = offset;
+    } else {
+        myoff = fd->fp_ind;
+    }
+    if (file_ptr_type == ADIO_INDIVIDUAL) {
+        myoff = fd->fp_ind;
+    }
+    plfs_debug( "%s: offset %ld len %ld rank %d\n",
+                myname, (long)myoff, (long)len, rank );
+    if ((fd->access_mode != ADIO_RDONLY) && fd->fs_ptr) {
+        // calls plfs_sync + barrier to ensure all ranks flush in-memory
+        // index before any rank calling plfs_read.
+        // need not do this for read-only file.
+        plfs_sync( fd->fs_ptr);
+        //we can't barrier here, MPI_File_read_at calls this function
+        //and is non-collective (we've seen a run hang where rank 0
+        //enters this function but rank 1 does not for example)
+        //plfs_barrier(fd->comm,rank);
+    }
+    err = plfs_read( fd->fs_ptr, buf, len, myoff );
+#ifdef HAVE_STATUS_SET_BYTES
+    if (err >= 0 ) {
+        MPIR_Status_set_bytes(status, datatype, err);
+    }
+#endif
+    if (err < 0 ) {
+        *error_code = MPIO_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE,
+                                           myname, __LINE__, MPI_ERR_IO,
+                                           "**io",
+                                           "**io %s", strerror(-err));
+    } else {
+        if (file_ptr_type == ADIO_INDIVIDUAL) {
+            fd->fp_ind += err;
+        }
+        *error_code = MPI_SUCCESS;
+    }
+#ifdef ROMIO_CRAY
+MPIIO_TIMER_END(RSYSIO);
+#endif /* ROMIO_CRAY */
+}
--- openmpi-1.4.3/ompi/mca/io/romio/romio/adio/ad_plfs/ad_plfs_resize.c	1969-12-31 16:00:00.000000000 -0800
+++ manio-ompi-1.4.3/ompi/mca/io/romio/romio/adio/ad_plfs/ad_plfs_resize.c	2012-11-29 14:17:04.225331098 -0800
@@ -0,0 +1,104 @@
+/* -*- Mode: C; c-basic-offset:4 ; -*- */
+/*
+ *   $Id: ad_plfs_resize.c,v 1.1 2010/11/29 19:59:01 adamm Exp $
+ *
+ *   Copyright (C) 1997 University of Chicago.
+ *   See COPYRIGHT notice in top-level directory.
+ */
+
+#include "ad_plfs.h"
+
+extern void reduce_meta(ADIO_File afd, Plfs_fd *fd, const char *filename,
+                        Plfs_close_opt *close_opt, int rank);
+
+void ADIOI_PLFS_Resize(ADIO_File fd, ADIO_Offset size, int *error_code)
+{
+    int err, rank, procs, amode, perm;
+    size_t bytes_written=0, total_bytes=0, flatten=0;
+    Plfs_open_opt open_opt;
+    Plfs_close_opt close_opt;
+    int file_is_open=1, do_close_reopen=0;
+    uid_t uid = geteuid();
+    static char myname[] = "ADIOI_PLFS_RESIZE";
+    plfs_debug( "%s: begin\n", myname );
+    /* don't permit resizing/truncating a read-only file */
+    if (fd->access_mode == ADIO_RDONLY) {
+        err = EPERM;
+        *error_code = MPIO_Err_create_code(err, MPIR_ERR_RECOVERABLE,
+                                           myname, __LINE__, MPI_ERR_READ_ONLY,
+                                           "**io",
+                                           "**io %s", strerror(err));
+        plfs_debug( "%s: cannot resize/truncate a read-only file.\n", myname );
+        return;
+    }
+    MPI_Comm_rank(fd->comm, &rank);
+    MPI_Comm_size( fd->comm, &procs);
+    // in container mode, we might need to close then reopen to ensure
+    // the truncate works correctly.  flat file doesn't need this though
+    if (plfs_get_filetype(fd->filename) == CONTAINER) {
+        /* do close+reopen when user ever wrote something. */
+        plfs_query(fd->fs_ptr, NULL, NULL, &bytes_written, NULL);
+        MPI_Allreduce(&bytes_written, &total_bytes, 1, MPI_LONG_LONG,
+                      MPI_SUM, fd->comm);
+        if (bytes_written) {
+            do_close_reopen = 1;
+        }
+        if (do_close_reopen) {
+            plfs_debug( "%s: do close+reopen for truncate.\n", myname );
+            close_opt.pinter=PLFS_MPIIO;
+            close_opt.num_procs = procs;
+            reduce_meta(fd, fd->fs_ptr, fd->filename, &close_opt, rank);
+            amode = ad_plfs_amode(fd->access_mode);
+            plfs_close(fd->fs_ptr, rank, uid, amode, &close_opt);
+            file_is_open = 0;
+            fd->fs_ptr = NULL;
+            plfs_barrier(fd->comm,rank);
+        }
+    }
+    /* do the truncate */
+    if (rank == fd->hints->ranklist[0]) {
+        // running the silverton test code we are seeing that
+        // rank 1 has already opened the file and then rank 0
+        // gets the Resize call and doesn't see that the file is open
+        // then we 0 calls this with file_is_open==0, plfs_trunc in
+        // container mode does unlink of droppings thereby destroying
+        // rank 1's open files.  Thus, let's always do open_file==1 here
+        // since we can't tell for sure that someone else doesn't have it
+        // open.  then plfs_trunc internal will only truncate droppings and
+        // not delete
+        file_is_open=1;
+        err = plfs_trunc(fd->fs_ptr, fd->filename, size, file_is_open);
+    }
+    // we want to barrier so that no-one leaves until we are done truncating
+    // we are relying on MPI_Bcast to do an effective barrier for us
+    MPI_Bcast(&err, 1, MPI_INT, fd->hints->ranklist[0], fd->comm);
+    if (err < 0) {
+        *error_code = MPIO_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE,
+                                           myname, __LINE__, MPI_ERR_IO,
+                                           "**io",
+                                           "**io %s", strerror(-err));
+    } else {
+        *error_code = MPI_SUCCESS;
+    }
+    /* do re-open if file was closed */
+    if (do_close_reopen) {
+        flatten = ad_plfs_hints (fd , rank, "plfs_flatten_close");
+        open_opt.pinter = PLFS_MPIIO;
+        open_opt.index_stream = NULL;
+        open_opt.reopen = 1;
+        open_opt.buffer_index = 0;
+        if (flatten != -1) {
+            open_opt.buffer_index = flatten;
+        }
+        perm = adplfs_getPerm(fd);
+        err = plfs_open( (Plfs_fd **)&(fd->fs_ptr), fd->filename, amode, rank,
+                         perm, &open_opt);
+        if ((err < 0) && (*error_code == MPI_SUCCESS)) {
+            *error_code = MPIO_Err_create_code(MPI_SUCCESS,
+                                               MPIR_ERR_RECOVERABLE,
+                                               myname, __LINE__, MPI_ERR_IO,
+                                               "**io",
+                                               "**io %s", strerror(-err));
+        }
+    }
+}
--- openmpi-1.4.3/ompi/mca/io/romio/romio/adio/ad_plfs/ad_plfs_write.c	1969-12-31 16:00:00.000000000 -0800
+++ manio-ompi-1.4.3/ompi/mca/io/romio/romio/adio/ad_plfs/ad_plfs_write.c	2012-11-29 14:17:04.225331098 -0800
@@ -0,0 +1,68 @@
+/* -*- Mode: C; c-basic-offset:4 ; -*- */
+/*
+ *   $Id: ad_plfs_write.c,v 1.1 2010/11/29 19:59:01 adamm Exp $
+ *
+ *   Copyright (C) 1997 University of Chicago.
+ *   See COPYRIGHT notice in top-level directory.
+ */
+
+#include "ad_plfs.h"
+#include "adio_extern.h"
+
+#ifdef ROMIO_CRAY
+#include "../ad_cray/ad_cray.h"
+#endif /* ROMIO_CRAY */
+
+void ADIOI_PLFS_WriteContig(ADIO_File fd, void *buf, int count,
+                            MPI_Datatype datatype, int file_ptr_type,
+                            ADIO_Offset offset, ADIO_Status *status,
+                            int *error_code)
+{
+    /* --BEGIN CRAY MODIFICATION-- */
+    int err=-1, datatype_size, rank;
+    ADIO_Offset len;
+    /* --END CRAY MODIFICATION-- */
+    ADIO_Offset myoff;
+    static char myname[] = "ADIOI_PLFS_WRITECONTIG";
+#ifdef ROMIO_CRAY
+MPIIO_TIMER_START(WSYSIO);
+#endif /* ROMIO_CRAY */
+    MPI_Type_size(datatype, &datatype_size);
+    /* --BEGIN CRAY MODIFICATION-- */
+    len = (ADIO_Offset)datatype_size * (ADIO_Offset)count;
+    /* --END CRAY MODIFICATION-- */
+    MPI_Comm_rank( fd->comm, &rank );
+    // for the romio/test/large_file we always get an offset of 0
+    // maybe we need to increment fd->fp_ind ourselves?
+    if (file_ptr_type == ADIO_EXPLICIT_OFFSET) {
+        myoff = offset;
+    } else {
+        myoff = fd->fp_ind;
+    }
+    if (file_ptr_type == ADIO_INDIVIDUAL) {
+        myoff = fd->fp_ind;
+    }
+    plfs_debug( "%s: offset %ld len %ld rank %d\n",
+                myname, (long)myoff, (long)len, rank );
+    err = plfs_write( fd->fs_ptr, buf, len, myoff, rank );
+#ifdef HAVE_STATUS_SET_BYTES
+    if (err >= 0 ) {
+        MPIR_Status_set_bytes(status, datatype, err);
+    }
+#endif
+    if (err < 0 ) {
+        *error_code = MPIO_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE,
+                                           myname, __LINE__, MPI_ERR_IO,
+                                           "**io",
+                                           "**io %s", strerror(-err));
+    } else {
+        if (file_ptr_type == ADIO_INDIVIDUAL) {
+            fd->fp_ind += err;
+        }
+        *error_code = MPI_SUCCESS;
+    }
+#ifdef ROMIO_CRAY
+MPIIO_TIMER_END(WSYSIO);
+#endif /* ROMIO_CRAY */
+}
+
--- openmpi-1.4.3/ompi/mca/io/romio/romio/adio/ad_plfs/Makefile.am	1969-12-31 16:00:00.000000000 -0800
+++ manio-ompi-1.4.3/ompi/mca/io/romio/romio/adio/ad_plfs/Makefile.am	2012-11-29 14:17:04.261331106 -0800
@@ -0,0 +1,34 @@
+#
+# Copyright (c) 2004-2005 The Trustees of Indiana University and Indiana
+#                         University Research and Technology
+#                         Corporation.  All rights reserved.
+# Copyright (c) 2004-2005 The University of Tennessee and The University
+#                         of Tennessee Research Foundation.  All rights
+#                         reserved.
+# Copyright (c) 2004-2005 High Performance Computing Center Stuttgart, 
+#                         University of Stuttgart.  All rights reserved.
+# Copyright (c) 2004-2005 The Regents of the University of California.
+#                         All rights reserved.
+# Copyright (c) 2008      Cisco Systems, Inc.  All rights reserved.
+# $COPYRIGHT$
+# 
+# Additional copyrights may follow
+# 
+# $HEADER$
+#
+
+include $(top_srcdir)/Makefile.options
+
+noinst_LTLIBRARIES = libadio_plfs.la
+libadio_plfs_la_SOURCES = \
+	ad_plfs.c \
+	ad_plfs.h \
+	ad_plfs_close.c \
+	ad_plfs_delete.c \
+	ad_plfs_fcntl.c \
+	ad_plfs_flush.c \
+	ad_plfs_open.c \
+	ad_plfs_read.c \
+	ad_plfs_resize.c \
+	ad_plfs_hints.c \
+	ad_plfs_write.c 
